1、了解卷积核的操作原理。

![image-20220109214425000](C:\Users\17675\AppData\Roaming\Typora\typora-user-images\image-20220109214425000.png)

​	[(12条消息) CNN卷积中多通道卷积的参数问题_dulingtingzi的博客-CSDN博客_多通道卷积](https://blog.csdn.net/dulingtingzi/article/details/79819513)

2、学习卷积之前看看经典的卷积网络。

​	https://blog.csdn.net/epubit17/article/details/78706921?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164820299716780265497510%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=164820299716780265497510&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-78706921.142

（1）Lenet-5

​	[(12条消息) 深度学习 --- 卷积神经网络CNN（LeNet-5网络详解）_zsffuture的博客-CSDN博客_lenet-5](https://blog.csdn.net/weixin_42398658/article/details/84392845)

（2）Alexnet

​	[深度学习卷积神经网络-AlexNet - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/42914388)

​	主要工作：

​        1、加入非线性ReLU函数。

​		2、多GPU加速训练。

​		3、局部响应归一化（**Local Response Normalization,LRN**）

​		4、重叠池化（**Overlapping Pooling**）

​		5、减少过拟合的操作：数据增强之镜像反射和随即裁剪、Dropout、随机梯度下降（动量衰减、权重衰减）

（3）VGG

​	[一文读懂VGG网络 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/41423739)

​	主要工作：1、采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）

​		如下图就是两个3x3代替5x5的卷积

![image-20220326093241316](C:\Users\17675\AppData\Roaming\Typora\typora-user-images\image-20220326093241316.png)

​		**VGG优点**

- VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。
- 几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好：
- 验证了通过不断加深网络结构可以提升性能。



​		**VGG缺点**

- VGG耗费更多计算资源，并且使用了更多的参数（这里不是3x3卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。VGG可是有3个全连接层啊！

​	PS：有的文章称：发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数		量。

​	注：很多pretrained的方法就是使用VGG的model（主要是16和19），VGG相对其他的方法，参数空间很	大，最终的model有500多m，AlexNet只有200m，GoogLeNet更少，所以train一个vgg模型通常要花费	更长的时间，所幸有公开的pretrained model让我们很方便的使用。

（4）GoogLeNet

​	[深度学习|经典网络：GoogLeNet（一） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/73857137)

​	主要工作：1、采用了Inception模块化结构

![image-20220326104216198](C:\Users\17675\AppData\Roaming\Typora\typora-user-images\image-20220326104216198.png)

​	2、网络最后采用了average pooling（平均池化）来代替全连接层，该想法来自NIN（Network in Network），事实证明这样可以将准		确率提高0.6%。

​	3、辅助分类器（减少梯度消失）

（5）ResNet（深度残差网络）

​	[你必须要知道CNN模型：ResNet - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/31852747)

（6）SE-Net(**Squeeze-and-Excitation Networks**)

​	主要工作：1、采用SE block的基本结构

​		![image-20220326112947752](C:\Users\17675\AppData\Roaming\Typora\typora-user-images\image-20220326112947752.png)

（7）NIN（Network in Network）

​	1、这篇论文中两个创新点：

​		(1) 引入mlpconv结构（1x1卷积的使用）.
​		(2) 使用全局平均池化（GAP）代替全连接层。
​	2、Mlpconv(1x1卷积) 的优点：
​		(1) 实现特征图的跨通道聚合
​		(2) 通过设置1x1卷积核的数量可以实现特征的降维或升维，这在之后的GoogLeNet中有所应用；同时可以减少网络参数数量。
​	3、全局平均池化：
​		(1) 有效减少参数数量（相比较全连接层）。
​		(2) 减轻过拟合。
​		(3) 更符合CNN特点，使feature map和类别信息产生直接映射，分类过程可理解性更强。
​		(4) 求和取平均操作综合了空间信息，使模型的鲁棒性更强。
​		(5) 缺点：对特征图简单的加权取平均操作可能会丢失一些有用信息。

​	![image-20220326141606571](C:\Users\17675\AppData\Roaming\Typora\typora-user-images\image-20220326141606571.png)
